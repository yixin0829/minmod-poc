{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
            "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from typing import Any\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "PATH = \"../data/raw/mvt_zinc/reports_processed/Bongar\u00e1 Zn 3-2019.pdf\"\n",
        "\n",
        "# Get elements\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=PATH,\n",
        "    # Unstructured first finds embedded image blocks\n",
        "    extract_images_in_pdf=False,\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 3800 chars\n",
        "    # Attempt to keep chunks > 2000 chars\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=\"../data/asset/extraction_unstructured\",\n",
        "    table_output_dir_path=\"../data/asset/extraction_unstructured\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 180,\n",
              " \"<class 'unstructured.documents.elements.Table'>\": 81,\n",
              " \"<class 'unstructured.documents.elements.TableChunk'>\": 4}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a dictionary to store counts of each type\n",
        "category_counts = {}\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "    category = str(type(element))\n",
        "    if category in category_counts:\n",
        "        category_counts[category] += 1\n",
        "    else:\n",
        "        category_counts[category] = 1\n",
        "\n",
        "# Unique_categories will have unique elements\n",
        "unique_categories = set(category_counts.keys())\n",
        "category_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "85\n",
            "180\n"
          ]
        }
      ],
      "source": [
        "class Element(BaseModel):\n",
        "    type: str\n",
        "    ele: Any\n",
        "\n",
        "\n",
        "# Categorize by type\n",
        "categorized_elements = []\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"table\", ele=element))\n",
        "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"text\", ele=element))\n",
        "\n",
        "# Tables\n",
        "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
        "print(len(table_elements))\n",
        "\n",
        "# Text\n",
        "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
        "print(len(text_elements))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<table><tr><td>37MP Concessions</td><td>Penalties:</td><td>$160.190</td><td>30</td><td>June, 2017</td></tr></table>\n"
          ]
        }
      ],
      "source": [
        "print(table_elements[20].ele.metadata.text_as_html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\ \n",
        "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "# Summary chain\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply to tables\n",
        "tables = [i.text for i in table_elements]\n",
        "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "minmod-poc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
